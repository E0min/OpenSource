{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/mecab-colab.sh | bash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv('../kr3.tsv', sep='\\t')\n",
    "\n",
    "# 부정(0)과 긍정(1)만 필터링\n",
    "df = df[df['Rating'].isin([0, 1])]\n",
    "\n",
    "# 부정 리뷰와 긍정 리뷰 분리\n",
    "negative_reviews = df[df['Rating'] == 0]\n",
    "positive_reviews = df[df['Rating'] == 1]\n",
    "\n",
    "# 각각에서 30,000개씩 무작위 샘플링\n",
    "negative_sampled = negative_reviews.sample(n=30000, random_state=42)\n",
    "positive_sampled = positive_reviews.sample(n=30000, random_state=42)\n",
    "\n",
    "# 샘플들을 합치기\n",
    "df_sampled = pd.concat([negative_sampled, positive_sampled]).reset_index(drop=True)\n",
    "\n",
    "# 중복 제거, 정규 표현식, NULL 값 제거\n",
    "df_sampled = df_sampled.drop_duplicates(subset=['Review']).dropna()\n",
    "df_sampled['Review'] = df_sampled['Review'].str.replace(\"[^가-힣 ]\", \"\")\n",
    "\n",
    "# KoNLPy의 Okt 객체 생성 및 토큰화\n",
    "okt = Okt()\n",
    "stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다']\n",
    "df_sampled['Tokenized'] = df_sampled['Review'].apply(lambda x: [word for word in okt.morphs(x, stem=True) if word not in stopwords])\n",
    "\n",
    "# 단어 카운팅\n",
    "words = Counter(sum(df_sampled['Tokenized'].tolist(), []))\n",
    "\n",
    "# 빈도수가 2회 이하인 단어 제거\n",
    "words = {word:freq for word, freq in words.items() if freq > 2}\n",
    "\n",
    "# word_to_index 매핑 생성\n",
    "word_to_index = {word: index + 1 for index, word in enumerate(words)}\n",
    "\n",
    "# 정수 인코딩\n",
    "df_sampled['Encoded'] = df_sampled['Tokenized'].apply(lambda x: [word_to_index.get(word, 0) for word in x])\n",
    "\n",
    "# 시퀀스 최대 길이 결정\n",
    "max_len = max(df_sampled['Encoded'].apply(lambda x: len(x)))\n",
    "\n",
    "# 시퀸스 최대 길이 저장\n",
    "with open('./max_len_1.pkl', 'wb') as f:\n",
    "    pickle.dump(max_len, f)\n",
    "\n",
    "# 패딩된 시퀀스 생성\n",
    "X_data = pad_sequences(df_sampled['Encoded'], maxlen=max_len, padding='post')\n",
    "\n",
    "with open('./paddedSequence_3.pkl', 'wb') as f:\n",
    "    pickle.dump(word_to_index, f)\n",
    "\n",
    "# word_to_index 매핑 저장\n",
    "with open('./word_to_index_3.pkl', 'wb') as f:\n",
    "    pickle.dump(word_to_index, f)\n",
    "  \n",
    "# 남은 단어 목록 출력\n",
    "print(\"남은 단어 목록:\")\n",
    "print(list(words.keys()))\n",
    "\n",
    "# 전처리한 데이터프레임을 TSV 파일로 저장\n",
    "df_sampled.to_csv('./preprocessed_kr3data_3.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리된 파일 불러오기\n",
    "file_path = './preprocessed_kr3data_3.tsv'\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# 정수 인코딩 및 패딩\n",
    "df['Encoded'] = df['Tokenized'].apply(lambda x: [word_to_index[word] for word in x if word in word_to_index])\n",
    "y_data = df['Rating'].values\n",
    "\n",
    "# 저장된 word_to_index 매핑 불러오기\n",
    "with open('./word_to_index_3.pkl', 'rb') as f:  # 'rb' 모드로 변경\n",
    "    word_to_index = pickle.load(f)\n",
    "\n",
    "# 저장된 max_len 값 불러오기\n",
    "with open('/content/drive/MyDrive/max_len_3.pkl', 'rb') as f:  # 'rb' 모드로 변경\n",
    "    max_len = pickle.load(f)\n",
    "\n",
    "with open('/content/drive/MyDrive/paddedSequence_3.pkl', 'rb') as f:\n",
    "    X_data = pickle.load(f)\n",
    "\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# 모델 구축 및 컴파일\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_to_index) + 1, 100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# EarlyStopping과 ModelCheckpoint 설정\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "model_checkpoint = ModelCheckpoint('./best_model_3.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(X_train, y_train, epochs=15, batch_size=64, validation_split=0.2, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# 예측 및 결과 조정 (0과 1이 아닌 경우 2 출력)\n",
    "predictions = model.predict(X_test)\n",
    "predictions = np.where(predictions >= 0.5, 1, 0)\n",
    "predictions = np.where((predictions != 0) & (predictions != 1), 2, predictions)\n",
    "\n",
    "# 테스트 데이터에 대한 모델 평가\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"테스트 정확도: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델 불러오기\n",
    "model = load_model('./best_model_3.h5')\n",
    "\n",
    "# KoNLPy의 Okt 객체 및 불용어 리스트\n",
    "okt = Okt()\n",
    "stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다']\n",
    "\n",
    "# 저장된 word_to_index 매핑 불러오기\n",
    "with open('./word_to_index_3.pkl', 'rb') as f:  # 'rb' 모드로 변경\n",
    "    word_to_index = pickle.load(f)\n",
    "\n",
    "# 저장된 max_len 값 불러오기\n",
    "with open('./max_len_3.pkl', 'rb') as f:  # 'rb' 모드로 변경\n",
    "    max_len = pickle.load(f)\n",
    "\n",
    "\n",
    "# 데이터 전처리 함수 정의\n",
    "def preprocess_review(review):\n",
    "    review = re.sub(\"[^가-힣 ]\", \"\", review)\n",
    "    tokenized = [word for word in okt.morphs(review, stem=True) if word not in stopwords]\n",
    "    encoded = [word_to_index.get(word, 0) for word in tokenized]\n",
    "    # 여기서 pad_sequences 함수를 사용\n",
    "    padded = pad_sequences([encoded], maxlen=max_len, padding='post')\n",
    "    return padded\n",
    "\n",
    "# 새로운 리뷰 데이터 예시\n",
    "new_review = '맛있어요 좋아요.'\n",
    "\n",
    "# 리뷰 데이터 전처리\n",
    "processed_review = preprocess_review(new_review)\n",
    "\n",
    "# 예측 수행\n",
    "prediction = model.predict(processed_review)\n",
    "predicted_rating = 1 if prediction[0][0] > 0.5 else 0\n",
    "\n",
    "# 결과 출력\n",
    "print(\"예측된 Rating:\", \"긍정\" if predicted_rating == 1 else \"부정\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
